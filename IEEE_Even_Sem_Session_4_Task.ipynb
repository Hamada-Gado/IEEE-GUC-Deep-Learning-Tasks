{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamada-Gado/IEEE-GUC-Deep-Learning-Tasks/blob/main/IEEE_Even_Sem_Session_4_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 4\n",
        "\n",
        "This task is all about CNNs (Convolutional Neural Networks) \n",
        "\n",
        "It's recommend to read [the relevant chapter in the textbook](https://d2l.ai/chapter_convolutional-neural-networks/index.html) in case you have trouble remembering certain concepts about how CNNs work.\n",
        "\n",
        "[This lecture](https://www.youtube.com/watch?v=bNb2fEVKeEo) is also great in case you prefer video. \n",
        "\n",
        "We will be using the GTSRB (German Traffic Sign Recognition Benchmark) dataset.\n",
        "\n",
        "The goal is to predict which traffic sign an image represents from 43 classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "AYfmnuy_q4Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As always, we start by importing the packages we need.\n",
        "\n",
        "Note: You'll notice here a new package called torchvision:\n",
        "> The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision"
      ],
      "metadata": {
        "id": "WTWWwuKP1o4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAt93YbFqz8o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, we'll download the dataset. The dataset we'll be using is actually directly available to download using torchvision\n",
        "\n",
        "Torchvision contains a lot of public datasets that can be installed directly using the package. If you need datasets from other sources though, you'll need to download them yourself."
      ],
      "metadata": {
        "id": "njgkO3qH1Qp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For more information see https://pytorch.org/vision/stable/datasets.html\n",
        "dataset = torchvision.datasets.GTSRB(root = \".\", download = True) # \".\" means current directory"
      ],
      "metadata": {
        "id": "DsabDXMFYJ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we've downloaded the dataset, let's inspect it. \n",
        "\n",
        "Do not that, unlike tabular datasets that can usually be represented in a single csv that you can read using pandas. Image datasets can have different formats and be organized in different ways. So a good idea is to check how the dataset is organized after downloading it and possibly reorganizing it."
      ],
      "metadata": {
        "id": "1LvRmEoG2UF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: we'll use shell commands to print folder contents and perform similar file system operations. We can do the same using python code but using shell commands can often be easier and more concise."
      ],
      "metadata": {
        "id": "IkuJByN62z85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints current working directory, can also instead use os.getcwd()\n",
        "!pwd"
      ],
      "metadata": {
        "id": "FgIzWUJc3GMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List contents of argument folder, can also instead type os.listdir(directory_path)\n",
        "\n",
        "!ls . # \".\" means current directory"
      ],
      "metadata": {
        "id": "UB0S_H2CYV0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After listing a few directories, we arrive at the directory with the training data. We find some numbered folders and a readme file. Let's see what it has to say! "
      ],
      "metadata": {
        "id": "fxeGXedY4YCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gtsrb/GTSRB/Training"
      ],
      "metadata": {
        "id": "_wO0lMgI3v5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat gtsrb/GTSRB/Training/Readme.txt"
      ],
      "metadata": {
        "id": "WJLBcEUA4lps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alright, seems like every folder in the previous directory is for a single class. There's also a csv file in every class directory that has additional labels but in this task we're only interested in classifying images into different classes so the folder name is enough as a label. "
      ],
      "metadata": {
        "id": "L1OBQ8QZ5nE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we know the directory where our data is placed and how it's organized. We can save the directory path to a variable to avoid typing it over and over. "
      ],
      "metadata": {
        "id": "2FvrdsBs6mbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dir = \"/content/gtsrb/GTSRB/Training/\""
      ],
      "metadata": {
        "id": "3jF6GKXH6z1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tip: we can also use python variables in shell commands when using notebooks! We need to wrap them in braces {}."
      ],
      "metadata": {
        "id": "ww4cN4OhTAaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {training_dir}"
      ],
      "metadata": {
        "id": "Tb2ubIAnTAAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {training_dir}/00000"
      ],
      "metadata": {
        "id": "5nXDOWSk7gp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now it's a good idea to inspect some images to see their size and verify they're correct."
      ],
      "metadata": {
        "id": "DtNhEPK563Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the class names we're predicting in order."
      ],
      "metadata": {
        "id": "Rrq3H_Bn9sy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Speed limit (20km/h)', \n",
        " 'Speed limit (30km/h)', \n",
        " 'Speed limit (50km/h)', \n",
        " 'Speed limit (60km/h)', \n",
        " 'Speed limit (70km/h)', \n",
        " 'Speed limit (80km/h)', \n",
        " 'End of speed limit (80km/h)',\n",
        " 'Speed limit (100km/h)', \n",
        " 'Speed limit (120km/h)',\n",
        " 'No passing',\n",
        " 'No passing veh over 3.5 tons',\n",
        " 'Right-of-way at intersection',\n",
        " 'Priority road', \n",
        " 'Yield',\n",
        " 'Stop',\n",
        " 'No vehicles',\n",
        " 'Veh > 3.5 tons prohibited',\n",
        " 'No entry',\n",
        " 'General caution',\n",
        " 'Dangerous curve left',\n",
        " 'Dangerous curve right',\n",
        " 'Double curve',\n",
        " 'Bumpy road',\n",
        " 'Slippery road',\n",
        " 'Road narrows on the right',\n",
        " 'Road work',\n",
        " 'Traffic signals',\n",
        " 'Pedestrians', \n",
        " 'Children crossing',\n",
        " 'Bicycles crossing',\n",
        " 'Beware of ice/snow',\n",
        " 'Wild animals crossing', \n",
        " 'End speed + passing limits',\n",
        " 'Turn right ahead',\n",
        " 'Turn left ahead',\n",
        " 'Ahead only',\n",
        " 'Go straight or right',\n",
        " 'Go straight or left',\n",
        " 'Keep right',\n",
        " 'Keep left',\n",
        " 'Roundabout mandatory',\n",
        " 'End of no passing',\n",
        " 'End no passing veh > 3.5 tons']\n",
        "\n",
        "num_classes = len(classes)"
      ],
      "metadata": {
        "id": "gvuXPVEM9teq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We'll use OpenCV to load images. OpenCV is the most popular python library for manipulating images (along with Pillow, but Pillow is written in Python while OpenCV is written in C and C++ so it's faster)"
      ],
      "metadata": {
        "id": "JKt-KrzpbxTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing OpenCV\n",
        "import cv2 \n",
        "#For selecting random directories and images\n",
        "import random \n",
        "\n",
        "#Choose a random class from the training directory\n",
        "random_class = random.choice(os.listdir(training_dir))\n",
        "#Choose a random image within the class \n",
        "random_image = random.choice(os.listdir(f\"{training_dir}/{random_class}\"))\n",
        "\n",
        "\n",
        "img = cv2.imread(f\"{training_dir}/{random_class}/{random_image}\")\n",
        "#OpenCV uses BGR instead of RGB so we need to convert it\n",
        "# https://stackoverflow.com/a/33787594\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.title(f\"Showing class: {classes[int(random_class[3:])]} | Image shape: {img.shape}\")\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "yDryv_5R7QK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we're ready to start preparing the dataset and the model for training. We'll start by preparing the dataset.\n",
        "\n",
        "> Transforms are common image transformations available in the torchvision.transforms module. They can be chained together using Compose."
      ],
      "metadata": {
        "id": "x_Wftt_gMxXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "#For more information see: https://pytorch.org/vision/main/transforms.html\n",
        "transform = transforms.Compose([\n",
        "    # You can add other transformations in this list\n",
        "    transforms.Resize((40,40)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#Use ImageFolder to load the dataset\n",
        "dataset = torchvision.datasets.ImageFolder(training_dir, transform=transform)\n",
        "#Split the dataset into training and validation sets\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
        "\n",
        "\n",
        "#Define dataloaders to load the images in batches of size batch_size\n",
        "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size, shuffle=True,\n",
        "                                         drop_last=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_ds, batch_size, shuffle=True,\n",
        "                                         drop_last=True)"
      ],
      "metadata": {
        "id": "GGLWwsTLY1aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can have a closer look at the dataloader to see how it differs from the dataset"
      ],
      "metadata": {
        "id": "32pMUoJ03jP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for X,y in train_dataloader:\n",
        "  # X has shape (batch_size, n_channels, height, width)\n",
        "  # y has shape (batch_size,)\n",
        "  # Each iteration provides a single batch till the dataset ends\n",
        "  print(X.shape, y)\n",
        "  break"
      ],
      "metadata": {
        "id": "NlXs4x9GoyhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can also inspect class frequencies from the dataset to see if some classes are underrepresented"
      ],
      "metadata": {
        "id": "hMuakyMX3i7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "classes_freqs = Counter(dataset.targets)\n",
        "\n",
        "plt.bar(classes_freqs.keys(), classes_freqs.values())"
      ],
      "metadata": {
        "id": "HbICztd918dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's define the network \n",
        "\n",
        "We'll be using a simple network with an architecture similar to [LeNet](https://d2l.ai/chapter_convolutional-neural-networks/lenet.html)"
      ],
      "metadata": {
        "id": "zPweqLPCmsOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 43\n",
        "\n",
        "net = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=5, padding=2), \n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #TODO: Add a nn.Conv2d layer with 6 input channels, \n",
        "            #      16 output channels, and a kernel_size=5\n",
        "\n",
        "            #TODO: Add a nn.ReLU activation\n",
        "\n",
        "            #TODO: Add a nn.MaxPool2d layer with kernel_size=2 and stride=2 \n",
        "\n",
        "            #TODO: Add a nn.Flatten() before using Linear layers\n",
        "\n",
        "            #TODO: Add a nn.Linear with input 1024 and output 512\n",
        "\n",
        "            #TODO: Add a nn.ReLU activation\n",
        "\n",
        "            #TODO: Add a nn.Linear with input 512 and output 256\n",
        "\n",
        "            #TODO: Add a nn.ReLU activation\n",
        "\n",
        "            #TODO: Add a nn.Linear with input 256 and output=num_classes\n",
        "\n",
        "            )\n",
        "\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d):\n",
        "        #TOOD: call torch.nn.init.xavier_uniform_ on layer.weight\n",
        "        \n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "#Initialize weights using Xavier transform\n",
        "net.apply(init_weights)"
      ],
      "metadata": {
        "id": "W5gQzs71NpeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 8\n",
        "\n",
        "\n",
        "#TODO: Select torch.optim.Adam and use net.parameters() as an argument \n",
        "#      For more information see: \n",
        "#      https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "optim = \n",
        "#TODO: Select nn.CrossEntropyLoss() as a loss function\n",
        "loss_fn = \n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #TODO: Call net.train() to start training mode\n",
        "    \n",
        "    train_correct = 0\n",
        "    train_loss = 0\n",
        "    for X, y in train_dataloader:\n",
        "        #TODO: Call optim.zero_grad to zero out the gradient\n",
        "\n",
        "        #TODO: call net on the input X to generate predictions\n",
        "        pred = \n",
        "        #TODO: Calculate loss using loss_fn on pred and y \n",
        "        loss = \n",
        "        #See: https://stackoverflow.com/a/61094330 \n",
        "        train_loss += loss.item() * X.size(0)\n",
        "        pred = torch.argmax(pred, dim=-1)\n",
        "        train_correct += (pred == y).float().sum()\n",
        "        #TODO: Call loss.backward and optim.step\n",
        "\n",
        "\n",
        "    #TODO: Put the network into eval mode\n",
        "    \n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    for X, y in val_dataloader:\n",
        "      #TODO: Generate predictions and calculate loss\n",
        "      pred = \n",
        "      loss = \n",
        "      \n",
        "      val_loss += loss.item() * X.size(0)\n",
        "      pred = torch.argmax(pred, dim=-1)\n",
        "      val_correct += (pred == y).float().sum()\n",
        "\n",
        "\n",
        "\n",
        "    train_loss /= len(train_ds)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_loss /= len(val_ds)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    train_accuracy = 100 * train_correct / len(train_ds)\n",
        "    val_accuracy = 100 * val_correct / len(val_ds)\n",
        "\n",
        "    print(f\"Epoch: {epoch} Train loss: {train_loss} Train accuracy: {train_accuracy} \\\n",
        "                           Val loss: {val_loss} Val accuracy: {val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "FB7Rdq8_M2d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can plot train and validation losses per epoch as well as accuracy"
      ],
      "metadata": {
        "id": "YYzI3hQpFMlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), train_losses, val_losses)\n",
        "plt.legend(['train loss', 'val loss'])"
      ],
      "metadata": {
        "id": "PPO6PstqiMcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's choose some random images from the validation set and see how our model performs."
      ],
      "metadata": {
        "id": "nECTiu931Tqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "X,y = next(iter(val_dataloader))\n",
        "\n",
        "pred = net(X)\n",
        "pred = torch.argmax(pred, dim=-1)\n",
        "\n",
        "i = 0\n",
        "for image,prediction, label in zip(X,pred, y):\n",
        "  plt.title(f\"Predicted: {classes[prediction]} Actual class: {classes[label]}\")\n",
        "  plt.imshow(  image.permute(1, 2, 0)  )\n",
        "  plt.figure(i+1)\n",
        "  i += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxXXJlv41SQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#That's it!\n",
        "\n",
        "You can go further by trying different datasets (e.g CIFAR-10, ImageNet), different architectures (e.g VGG, ResNets), and different hyperparameters, image augmentation methods, and by seeing different Kaggle Image competitions and reading different solution.  "
      ],
      "metadata": {
        "id": "xWNyxO2MMlEQ"
      }
    }
  ]
}