{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamada-Gado/IEEE-GUC-Deep-Learning-Tasks/blob/main/IEEE_Session_3_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>In this notebook, we're going to use Pytorch to train a few classifiers for the Fashion-MNIST dataset.</h3>\n",
        "\n",
        "The classifiers we'll implement are: \n",
        "\n",
        "1. Linear Classifier (Logistic Regression as a Neural Network)\n",
        "\n",
        "2. Deep Neural Network (Adding non-linearity via an activation function)\n",
        "\n",
        "3. A DNN with a different parameter initialization (Xavier initialization)\n",
        "\n",
        "4. A DNN with l2 regularization (weight decay)\n",
        "\n",
        "5. A DNN with dropout\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<p>While these are a total of 5 classifiers, the differences in implementation are a few lines at most, sometimes even a single argument!\n",
        "\n",
        "Note: It's generally unadvisable to flatten images into a single dimension and treat them as a 1D vector without exploiting the spatial structrue in the image in a more satisfying way. \n",
        "\n",
        "We do this here for practice and for educational purposes, but a better way is to use Convolutional Neural Networks, which we discuss later.</p>\n",
        "\n",
        "Also note that all networks will take from 2 to 10 minutes to train. This is because we're training on CPU which generally uses much more time for neural network training than a GPU."
      ],
      "metadata": {
        "id": "QJAEpc9Dl1LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by importing PyTorch and some additional packages we'll need:"
      ],
      "metadata": {
        "id": "wjcVBKkeM1gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjwdORfGlgzH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# set default device based on CUDA's availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Class"
      ],
      "metadata": {
        "id": "C7MbFRs5ZCvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll define our dataset class. The dataset we're using is Fashion-MNIST, which is included in torchvision's datasets. We just have to call the appropiate function to downlaod it.\n",
        "\n",
        "We also make use of torchvision's transforms module to prepare the images by resizing them if needed and converting the to Pytorch tensors.\n",
        "\n",
        "We use torch's built-in DataLoader to get dataloaders for the training and validation sets and add some helper methods for visualizing the dataset."
      ],
      "metadata": {
        "id": "ak9r4tAa2Yb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST(Dataset):\n",
        "    \"\"\"The Fashion-MNIST dataset.\"\"\"\n",
        "    def __init__(self, batch_size=64, resize=(28, 28), root=\".\", num_workers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.resize = resize\n",
        "        self.root = root\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        trans = transforms.Compose([transforms.Resize(resize),\n",
        "                                    transforms.ToTensor()])\n",
        "        self.train = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=True, transform=trans, download=True)\n",
        "        self.val = torchvision.datasets.FashionMNIST(\n",
        "            root=self.root, train=False, transform=trans, download=True)\n",
        "\n",
        "\n",
        "    def text_labels(self, indices):\n",
        "      \"\"\"Return text labels.\"\"\"\n",
        "      labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "      return [labels[int(i)] for i in indices]\n",
        "\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        data = self.train if train else self.val\n",
        "        return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n",
        "                                          num_workers=self.num_workers)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.get_dataloader(train=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader(train=False)\n",
        "\n",
        "    def show_images(self, imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
        "        figsize = (num_cols * scale, num_rows * scale)\n",
        "        _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "        axes = axes.flatten()\n",
        "        for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
        "            try:\n",
        "                img = np.array(img)\n",
        "            except:\n",
        "                pass\n",
        "            ax.imshow(img)\n",
        "            ax.axes.get_xaxis().set_visible(False)\n",
        "            ax.axes.get_yaxis().set_visible(False)\n",
        "            if titles:\n",
        "                ax.set_title(titles[i])\n",
        "        return axes\n",
        "\n",
        "    def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
        "        X, y = batch\n",
        "        if not labels:\n",
        "            labels = self.text_labels(y)\n",
        "        self.show_images(X.squeeze(1), nrows, ncols, titles=labels)"
      ],
      "metadata": {
        "id": "pB1Ha7vAkjuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can instantiate the dataset class and get some batches of data and visualize them."
      ],
      "metadata": {
        "id": "nZSpfmNuO3X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = FashionMNIST()\n",
        "#Total number of training, validation examples\n",
        "len(data.train), len(data.val)"
      ],
      "metadata": {
        "id": "awJmCK9MlZXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shape of a single image\n",
        "data.train[0][0].shape"
      ],
      "metadata": {
        "id": "nEdEzRrPlbs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "#Shape of a single batch with batch size = 64\n",
        "print(X.shape, X.dtype, y.shape, y.dtype)"
      ],
      "metadata": {
        "id": "dtBnL_dxlqn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(data.val_dataloader()))\n",
        "data.visualize(batch)"
      ],
      "metadata": {
        "id": "UXvwafYvmnAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First classifier: Linear Network"
      ],
      "metadata": {
        "id": "kwGtPDZ9bs3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by defining our first classifier: a simple logistic regression model implemented as a neural network with a single linear layer.\n",
        "\n",
        "\n",
        "Note that in PyTorch we don't explicitly include the nn.Softmax in the net architecture. This is because PyTorch's built-in [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) expects the input to be unnormalized logits (i.e normal linear layer output without applying softmax)\n",
        "\n",
        "We don't need to apply Softmax during validation unless we require class probabilities, otherwise we can just get the maximum probability class with argmax in which case we don't need to calculate the Softmax before (as Softmax preserves ordering so the max argument before Softmax is the same after)\n",
        "\n"
      ],
      "metadata": {
        "id": "_xoJnXckPD5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: Implement a linear neural network for classification**"
      ],
      "metadata": {
        "id": "658hFaXhhPs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(nn.Module):  \n",
        "    def __init__(self, num_outputs, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_outputs=num_outputs\n",
        "        self.lr = lr\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "          #TODO: Add a nn.Flatten() layer\n",
        "\n",
        "          #TODO: Add a nn.Linear layer after it with input 28*28\n",
        "          #      and output=num_outputs\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "      Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "      Y = Y.reshape((-1,))\n",
        "      return F.cross_entropy(\n",
        "          Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "2-TjghOsswRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now what remains is to define the trainer class. Note that you can directly write the training loop without wrapping it in a class, but doing it this way is cleaner and more modular. We will use this same trainer class to train all classifiers in this notebook without having to rewrite it."
      ],
      "metadata": {
        "id": "ZbpkokYRQx89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: Complete the Trainer class**"
      ],
      "metadata": {
        "id": "pPg4mT0ti1u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, max_epochs, num_gpus=0):\n",
        "        assert num_gpus == 0, 'No GPU support yet'\n",
        "\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        #We use those lists to plot the loss and accuracy\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.train_acc = []\n",
        "        self.val_acc = []\n",
        "\n",
        "\n",
        "    def prepare_data(self, data):\n",
        "\n",
        "      self.train_dataloader = data.train_dataloader()\n",
        "      self.val_dataloader = data.val_dataloader()\n",
        "\n",
        "      self.num_train_batches = len(self.train_dataloader)\n",
        "      self.num_val_batches = (len(self.val_dataloader)\n",
        "                              if self.val_dataloader is not None else 0)\n",
        "        \n",
        "\n",
        "    def fit(self, model, data):\n",
        "\n",
        "        self.prepare_data(data)\n",
        "        self.model = model\n",
        "        self.optim = model.configure_optimizers()\n",
        "\n",
        "        self.epoch = 0\n",
        "        self.batch_size = data.batch_size\n",
        "\n",
        "        #Train for max_epochs epochs\n",
        "        for self.epoch in range(self.max_epochs):\n",
        "            self.fit_epoch()\n",
        "\n",
        "        self.plot_loss()\n",
        "        self.plot_accuracy()\n",
        "        self.visualize_wrong()\n",
        "\n",
        "    def fit_epoch(self):\n",
        "        #TODO: Call the train() method of self.model\n",
        "        \n",
        "\n",
        "        running_loss = 0\n",
        "        running_acc = 0\n",
        "\n",
        "        for batch in self.train_dataloader:\n",
        "\n",
        "            X, y = batch\n",
        "\n",
        "            #TODO: Calculate y_hat by applying self.model on X\n",
        "            y_hat = \n",
        "\n",
        "\n",
        "            #TODO: Calculate loss by calling self.model.loss\n",
        "            loss = \n",
        "\n",
        "\n",
        "            #Accumulate loss each batch in running_loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #TODO: Get predictions by calling torch.argmax on y_hat with axis=1\n",
        "            predicted = \n",
        "\n",
        "            #Calculate average accuracy for a single batch\n",
        "            running_acc += (predicted == y).sum().item() / self.batch_size\n",
        "\n",
        "            #TODO: Zero out self.optim's gradient with the zero_grad() method\n",
        "            \n",
        "\n",
        "\n",
        "            #TODO: Call loss.backward \n",
        "\n",
        "\n",
        "            #TODO: Call self.optim's step method\n",
        "\n",
        "\n",
        "        #Divide loss and accuracy by number of training batches per epoch\n",
        "        running_loss /= self.num_train_batches\n",
        "        running_acc /= self.num_train_batches\n",
        "\n",
        "        self.train_losses.append(running_loss)\n",
        "        self.train_acc.append(running_acc)\n",
        "\n",
        "        #TODO: Call the eval() method of self.model\n",
        "\n",
        "        \n",
        "\n",
        "        val_running_loss = 0\n",
        "        val_running_acc = 0\n",
        "\n",
        "        for batch in self.val_dataloader:\n",
        "            with torch.no_grad():\n",
        "\n",
        "                X, y = batch\n",
        "\n",
        "                #TODO: Calculate y_hat by applying self.model on X\n",
        "                y_hat = \n",
        "                #TODO: Calculate loss by calling self.model.loss\n",
        "                loss = \n",
        "                #TODO: Get predictions by calling torch.argmax on y_hat with axis=1    \n",
        "                predicted = \n",
        "\n",
        "                val_running_acc += (predicted == y).sum().item() / self.batch_size\n",
        "\n",
        "                val_running_loss += loss.item()\n",
        "        \n",
        "        val_running_loss /= self.num_val_batches\n",
        "        val_running_acc /= self.num_val_batches\n",
        "\n",
        "        self.val_losses.append(val_running_loss)\n",
        "        self.val_acc.append(val_running_acc)\n",
        "\n",
        "        print(f\"Epoch: {self.epoch}, train loss: {running_loss}, val loss: {val_running_loss} \\\n",
        "                train acc: {running_acc} val acc: {val_running_acc}\")\n",
        "\n",
        "\n",
        "    def plot_loss(self):\n",
        "      \"\"\"Plot loss for each epoch\"\"\"\n",
        "      plt.plot(self.train_losses, label='train loss')\n",
        "      plt.plot(self.val_losses, label='val loss', )\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    def plot_accuracy(self):\n",
        "      \"\"\"Plot accuracy for each epoch\"\"\"\n",
        "      plt.plot(self.train_acc, label='train accuracy')\n",
        "      plt.plot(self.val_acc, label='val accuracy')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    def visualize_wrong(self):\n",
        "      \"\"\"Visualize wrong examples for a single batch\n",
        "          The first line is the true class and \n",
        "          the second is the predicted class\"\"\"\n",
        "      X, y = next(iter(self.val_dataloader))\n",
        "      preds = self.model(X).argmax(axis=1)\n",
        "\n",
        "      wrong = preds.type(y.dtype) != y\n",
        "      X, y, preds = X[wrong], y[wrong], preds[wrong]\n",
        "      labels = [a+'\\n'+b for a, b in zip(\n",
        "          data.text_labels(y), data.text_labels(preds))]\n",
        "      data.visualize([X, y], labels=labels)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "VgU5zQg0tZM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the training part. We instantiate the dataset, model, and trainer classes, and call the fit method of the trainer class on the model and the data."
      ],
      "metadata": {
        "id": "5aK034ihRUY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Initialize the dataset class, model, trainer, and finall call the fit method of the trainer**"
      ],
      "metadata": {
        "id": "C8iZV0WCkAOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = #TODO: Instantiate the dataset class defined above with batch_size=256\n",
        "model = #TODO: Instantiate the model class with num_outputs=10 and lr=0.1\n",
        "trainer = #TODO: Instantiate the trainer class with max_epochs = 10\n",
        "\n",
        "\n",
        "#TODO: Call the fit method of the trainer class\n"
      ],
      "metadata": {
        "id": "GV3_NXALtRH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second Classifier: Multilayer Perceptron (MLP) "
      ],
      "metadata": {
        "id": "pU2SLAGacDtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: Implement MLP by adding a hidden layer and a ReLU activation to the Linear Model**"
      ],
      "metadata": {
        "id": "cSw4lw6ifN29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_num_outputs = num_outputs\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.lr = lr\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "        #TODO: Add a nn.Flatten()\n",
        "        #TODO: Add a nn.Linear with input=28*28 and output=num_hiddens\n",
        "\n",
        "        #TODO: Add a nn.ReLU after the Linear Layer you define\n",
        "        #TODO: Add a nn.Linear with input=num_hiddens and output=num_outputs\n",
        "                                 \n",
        "                                 )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "      Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "      Y = Y.reshape((-1,))\n",
        "      return F.cross_entropy(\n",
        "          Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "data = FashionMNIST(batch_size=256)\n",
        "model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "trainer = Trainer(max_epochs=10)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "NwgP6xyawqv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Third Classifier: MLP with Xavier initialization"
      ],
      "metadata": {
        "id": "CETrsYorcML9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5: Add Xavier initialization to the MLP**"
      ],
      "metadata": {
        "id": "qAP58LZvepE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply Xavier initialization we define the net as usual and then call the apply method of the net which applies its argument to all layers in the model. \n",
        "\n",
        "We check if the layer in question is a Linear layer and if so we call `torch.nn.init.xavier_uniform_` on layer.weight to update it in place."
      ],
      "metadata": {
        "id": "2FOa_CfesBr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPXavier(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens, lr):\n",
        "      super().__init__()\n",
        "\n",
        "      self.num_num_outputs = num_outputs\n",
        "      self.num_hiddens = num_hiddens\n",
        "      self.lr = lr\n",
        "\n",
        "      #TODO: Use the same net you used in Q4\n",
        "      self.net = nn.Sequential(\n",
        "          \n",
        "\n",
        "      )\n",
        "      \n",
        "      #TODO: Call self.net.apply and pass the init_weights method as an argument\n",
        "      \n",
        "\n",
        "    def init_weights(self, layer):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            #TODO: call torch.nn.init.xavier_uniform_ on layer.weight\n",
        "\n",
        "            layer.bias.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "      Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "      Y = Y.reshape((-1,))\n",
        "      return F.cross_entropy(\n",
        "          Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "\n",
        "data = FashionMNIST(batch_size=256)\n",
        "model = MLPXavier(num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "trainer = Trainer(max_epochs=10)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "W1sIM6qQyR2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fourth Classifier: MLP with weight decay (l2 regularization)"
      ],
      "metadata": {
        "id": "jwchCGxUcQ2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6: Add weight decay to the MLP model**"
      ],
      "metadata": {
        "id": "lZxg70jEdOaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight decay helps regularize the model by constraining the values of the weights. We can apply weight decay in Pytorch throught the weight_decay argument of the SGD optimizer."
      ],
      "metadata": {
        "id": "Rbkx9C1ruZzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPWeightDecay(nn.Module):\n",
        "    #TODO: Add a wd parameter to the init method with a default value of 0.001\n",
        "    def __init__(self, num_outputs, num_hiddens, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_num_outputs = num_outputs\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.lr = lr\n",
        "        #TODO: Add the wd parameter as a class attribute to use in the optimizer\n",
        "\n",
        "        #TODO: Use the same net you used in Q4 and Q5\n",
        "        self.net = nn.Sequential(\n",
        "\n",
        "                                  )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "      Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "      Y = Y.reshape((-1,))\n",
        "      return F.cross_entropy(\n",
        "          Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #TODO: Add a weight_decay argument to the SGD call with value self.wd\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "data = FashionMNIST(batch_size=256)\n",
        "model = MLPWeightDecay(num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "trainer = Trainer(max_epochs=10)\n",
        "trainer.fit(model, data)\n"
      ],
      "metadata": {
        "id": "_Gs67QqRy1uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fifth Classifier: MLP with dropout"
      ],
      "metadata": {
        "id": "dhZmGs5YcVu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7: Add dropout to the MLP model**"
      ],
      "metadata": {
        "id": "zXc5HdTiclp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use dropout to turn off some neurons with a probability p to regularize the model and reduce overfitting."
      ],
      "metadata": {
        "id": "NVy4UEoXtpYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPDropout(nn.Module):\n",
        "    #TODO: Add a dropout parameter with a default value of 0.5\n",
        "    def __init__(self, num_outputs, num_hiddens, lr):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_num_outputs = num_outputs\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.lr = lr\n",
        "        \n",
        "        #TODO: Use the same net in Q4, Q5, and Q6\n",
        "\n",
        "        #TODO: Add a nn.Dropout(p) after the ReLU activation\n",
        "        #      where p is the dropout parameter\n",
        "        self.net = nn.Sequential(\n",
        "            \n",
        "                                 )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "      Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
        "      Y = Y.reshape((-1,))\n",
        "      return F.cross_entropy(\n",
        "          Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "data = FashionMNIST(batch_size=256)\n",
        "model = MLPDropout(num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "trainer = Trainer(max_epochs=10)\n",
        "trainer.fit(model, data)\n"
      ],
      "metadata": {
        "id": "nWs2FWPx1D1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>That's it!</h1>\n",
        "This is optional, but feel free to change the number of hidden units in each hidden layer, change various hyperparameters like dropout probability, learning rate, batch size, and number of epochs. \n",
        "\n",
        "You can also experiment with adding more hidden layers and seeing how the loss and accuracy change. Does increasing regularization help when increasing the number of hidden layers?"
      ],
      "metadata": {
        "id": "1_CrBcH8sYL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribution: Code in this notebook was based on code from [Dive Into Deep Learning](https://d2l.ai/)"
      ],
      "metadata": {
        "id": "zRXcFc15pFrZ"
      }
    }
  ]
}