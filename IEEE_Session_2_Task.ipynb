{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamada-Gado/IEEE-GUC-Deep-Learning-Tasks/blob/main/IEEE_Session_2_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to use Pytorch to train a linear regression model. \n",
        "\n",
        "This task won't be exploring the real power of deep neural networks (we won't have any hidden layers so it's not \"deep\" and it can't represent nonlinear relationships in the data.\n",
        "\n",
        "The code, however, is very much similar to what deep neural network code looks like, that is, most of the code you use here you will be useful when writing code for a deep neural network. \n",
        "\n",
        "This task just helps get you familiar with the main steps of defining and training a neural network in Pytorch."
      ],
      "metadata": {
        "id": "QJAEpc9Dl1LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by importing PyTorch and its related packages "
      ],
      "metadata": {
        "id": "wjcVBKkeM1gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjwdORfGlgzH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set default device based on CUDA's availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll generate a synthetic dataset with known parameters so we can test if out Linear Regression Neural Network model will find the same parameters (which means close enough parameters since the model can't find the exact same parameters due to numerical precision)\n",
        "\n",
        "Do note that the target variable we're predicting is not the exact same as the function we have defined. This is because we inject Gaussian noise to the target variable Why? Because when observing a variable in the real world there's always a chance for observation error (like taking measurements using a microscope). We account for those changes (noise) by adding random numbers to each example, sampled from a normal (Gaussian) distrubution.\n",
        "\n",
        "We here choose a mean of 0 and a standard deviation of 0.1. Which means we multiply the output of torch.randn (which has a mean of 0 and a standard deviation of 1) by 0.1."
      ],
      "metadata": {
        "id": "8O54qC3JNV9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: Define the X, Y and noise variables**"
      ],
      "metadata": {
        "id": "ypdgiwpNvk8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = 2 #Number of features we'll generate\n",
        "num_outputs = 1 #We only have one output\n",
        "num_examples = 10_000 #We'll generate 10,000 examples\n",
        "dtype = torch.float\n",
        "\n",
        "#Define a function with 2 and -3.4 as parameters, 4.2 as bias\n",
        "def real_fn(X):\n",
        "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
        "\n",
        "#TODO: Generate training dataset consisting of random numbers\n",
        "#Use torch.randn to generate an array of shape (num_examples, num_inputs)\n",
        "X = \n",
        "\n",
        "\n",
        "#TODO: Add some Gaussian noise to the dataset\n",
        "#Use torch.randn to generate an array of shape (num_examples)\n",
        "#Multiply that array by 0.1 to change the standard deviation of the noise to .1\n",
        "noise = \n",
        "#TODO: Generate target variable using our function and the noise\n",
        "# y is the output of applying real_fn to X and adding the noise\n",
        "# Also reshape y to be of shape (-1, 1)\n",
        "y = "
      ],
      "metadata": {
        "id": "olsIAPU0LvfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can inspect the values of our training dataset and target variable, as well the true function we're predicting (without the added noise)"
      ],
      "metadata": {
        "id": "HtKCz1TeuziE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0]) #Sample from training set\n",
        "print(y[0]) #Corresponding target variable\n",
        "print(2 * X[0, 0] - 3.4 * X[0, 1] + 4.2) #Actual function value\n"
      ],
      "metadata": {
        "id": "B1akcJ3-Lxv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot our training dataset against the target variable"
      ],
      "metadata": {
        "id": "1HpKFZrQyV4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:, 0].cpu().numpy(), y.cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GRUDT0s5LzuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll define a very simple class to load the data, it loads the data, then defines the `__len__` method to get its size and the `__getitem__` method to get a single example and its ground truth"
      ],
      "metadata": {
        "id": "b7CK9iPFlv60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        assert X.size()[0] == y.size()[0]\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return X.size()[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return [X[idx], y[idx]]\n"
      ],
      "metadata": {
        "id": "wmgfmh8uL00K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use PyTorch's `DataLoader` class to generate the dataloader for our training data. We give it as input an instance of our dataset and specify the `batch_size` and set `shuffle=True` so that it provides batches in random order. "
      ],
      "metadata": {
        "id": "57zJc2_QyfK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "train_dataloader = DataLoader(LinearDataset(X, y), batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "YDy_UQnNL3EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now loop through the dataloader to get batches of data at each iteration. "
      ],
      "metadata": {
        "id": "-0nQ2OSHy5c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (data, label) in enumerate(train_dataloader):\n",
        "    print(data)\n",
        "    print(label)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "P8O6YtB0L4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do note that each time we loop through the data, it will have different order. That's because we've set shuffle=True. This helps the model become more robust as it gets batches in a different order in each epoch (pass through the training data so it reduces overfitting."
      ],
      "metadata": {
        "id": "KjXUd5SbzHIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (data, label) in enumerate(train_dataloader):\n",
        "    print(data)\n",
        "    print(label)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "D-gohynkL6Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we count how many iterations we go through in train_dataloader we find that it does 2500 iterations. That's because we have a total of 10,000 examples and a `batch_size` of 4. Changing the batch size should affect the number of iterations, which are generally `number of examples / batch_size`"
      ],
      "metadata": {
        "id": "ICuWFKaPzng9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i, (data, label) in enumerate(train_dataloader):\n",
        "    pass\n",
        "print(i + 1)\n"
      ],
      "metadata": {
        "id": "FoC3d55vL75M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: Define the Linear Regression model using Pytorch**"
      ],
      "metadata": {
        "id": "ITiZ2_EI0BfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(LinearRegression, self).__init__(**kwargs)\n",
        "        #TODO: Use torch's nn.Linear to define a Linear layer \n",
        "        #The layer should have an input of shape 2 and an output of shape 1 \n",
        "        self.dense_1 = \n",
        "        \n",
        "    def forward(self, x):\n",
        "        #TODO: Call the self.dense_1 you just defined on x\n",
        "        x = \n",
        "        return x\n",
        "    \n",
        "net = #TODO: Instantiate the LinearRegression class you just defined\n",
        "net.to(device)\n"
      ],
      "metadata": {
        "id": "eghnXrEKL9Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Use Mean Squared Error loss**"
      ],
      "metadata": {
        "id": "04eG0Fel1CBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = #TODO: use nn.MSELoss for the loss function\n"
      ],
      "metadata": {
        "id": "SQuFy87QL-gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: Use Stochastic Gradient Descent for optimizing the model**"
      ],
      "metadata": {
        "id": "thAjLAkH1HU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = #TODO: Use optim.SGD with learning rate = 1e-5 and a momentum of 0.9"
      ],
      "metadata": {
        "id": "3S-pwRXSL_vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5: Complete the training loop and train the model**"
      ],
      "metadata": {
        "id": "jUePzAzU1Vo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "num_batches = num_examples / batch_size\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    cumulative_loss = 0\n",
        "    # inner loop\n",
        "    for i, (data, label) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        yhat = net(data)\n",
        "        loss = #TODO: Call the loss function you defined earlier\n",
        "        \n",
        "        #TODO: Call the zero_grad method of the optimizer you defined\n",
        "        #TODO: Call the backward method of the loss tensor you just calculated\n",
        "        #TODO: Call the step method of the optimizer \n",
        "        cumulative_loss += loss.item()\n",
        "    print(\"Epoch %s, loss: %s\" % (epoch, cumulative_loss / num_examples))\n",
        "    losses.append(cumulative_loss / num_batches)\n"
      ],
      "metadata": {
        "id": "tGjrvueiMArs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can plot the average loss of the model for each epoch it trained."
      ],
      "metadata": {
        "id": "vZ3Xc6Fv2Q5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(num=None, figsize=(8, 6))\n",
        "plt.plot(losses)\n",
        "\n",
        "plt.grid(True, which=\"both\")\n",
        "plt.xlabel('epoch', fontsize=14)\n",
        "plt.ylabel('average loss', fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uABlACFUMCHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we print the parameters of the model. Notice that while it didn't get the exact values we'd set (2,-3.4, and 4.2). It got very close. Of course this is becuase this is a synthetic dataset that's completely linear. Real-life datasets are never this clean, but this serves as a demonstration and introduction to Pytorch and how it can model Linear Regression."
      ],
      "metadata": {
        "id": "1enpyDeo2Xwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = net.parameters()\n",
        "print('The type of \"params\" is a ', type(params))\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n"
      ],
      "metadata": {
        "id": "ybp28OMyMFy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribution: This task is based on this [notebook](https://github.com/Kulbear/pytorch-the-hard-way/blob/master/Linear%20Regression.ipynb)."
      ],
      "metadata": {
        "id": "zh4nKe-O4aqU"
      }
    }
  ]
}